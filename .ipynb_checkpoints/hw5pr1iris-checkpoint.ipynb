{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# hw5pr1iris:  iris clasification via decision trees and random forests...\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I predict setosa (0) from Features [4.6, 3.6, 3.0, 0.2]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# We don't need any data at all to create a decision tree!\n",
    "#\n",
    "import random\n",
    "\n",
    "def predictive_model( Features ):\n",
    "    \"\"\" input: a list of four features \n",
    "                [ sepallen, sepalwid, petallen, petalwid ]\n",
    "        output: the predicted species of iris, from\n",
    "                  setosa (0), versicolor (1), virginica (2)\n",
    "    \"\"\"\n",
    "    [ sepallen, sepalwid, petallen, petalwid ] = Features \n",
    "    # This was already a decision tree (or, a very small \"forest of one\"!)\n",
    "    if petalwid < 1.0:\n",
    "        return 'setosa (0)'\n",
    "    else:\n",
    "        return random.choice( ['versicolor (1)', 'virginica (2)'] )\n",
    "    \n",
    "#\n",
    "# Try it!\n",
    "# \n",
    "# Features = eval(input(\"Enter new Features: \"))\n",
    "#\n",
    "Features = [ 4.6, 3.6, 3.0, 0.2 ] \n",
    "result = predictive_model( Features )\n",
    "print(f\"I predict {result} from Features {Features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Let's use the data to create \"more informed\" models\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries!\n",
    "import numpy as np      # numpy is Python's \"array\" library\n",
    "import pandas as pd     # Pandas is Python's \"data\" library (\"dataframe\" == spreadsheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris.csv : file read into a pandas dataframe.\n"
     ]
    }
   ],
   "source": [
    "# let's read in our flower data...\n",
    "# \n",
    "# for read_csv, use header=0 when row 0 is a header row\n",
    "# \n",
    "filename = 'iris.csv'\n",
    "df = pd.read_csv(filename, header=0)   # encoding=\"latin1\" et al.\n",
    "print(f\"{filename} : file read into a pandas dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepallen</th>\n",
       "      <th>sepalwid</th>\n",
       "      <th>petallen</th>\n",
       "      <th>petalwid</th>\n",
       "      <th>irisname</th>\n",
       "      <th>adapted from https://en.wikipedia.org/wiki/Iris_flower_data_set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000e-01</td>\n",
       "      <td>setosa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>setosa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.000000e-01</td>\n",
       "      <td>setosa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.000000e-01</td>\n",
       "      <td>setosa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>2.000000e-01</td>\n",
       "      <td>setosa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.200000e+00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.300000e+00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>4.2</td>\n",
       "      <td>42.0</td>\n",
       "      <td>4242.0</td>\n",
       "      <td>4.200000e+42</td>\n",
       "      <td>alieniris</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>4.2</td>\n",
       "      <td>42.0</td>\n",
       "      <td>4242.0</td>\n",
       "      <td>4.200000e+42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepallen  sepalwid  petallen      petalwid   irisname  \\\n",
       "0         4.6       3.6       1.0  2.000000e-01     setosa   \n",
       "1         4.3       3.0       1.1  1.000000e-01     setosa   \n",
       "2         5.0       3.2       1.2  2.000000e-01     setosa   \n",
       "3         5.8       4.0       1.2  2.000000e-01     setosa   \n",
       "4         4.4       3.0       1.3  2.000000e-01     setosa   \n",
       "..        ...       ...       ...           ...        ...   \n",
       "138       7.7       3.8       6.7  2.200000e+00  virginica   \n",
       "139       7.7       2.8       6.7  2.000000e+00  virginica   \n",
       "140       7.7       2.6       6.9  2.300000e+00  virginica   \n",
       "141       4.2      42.0    4242.0  4.200000e+42  alieniris   \n",
       "142       4.2      42.0    4242.0  4.200000e+42        NaN   \n",
       "\n",
       "     adapted from https://en.wikipedia.org/wiki/Iris_flower_data_set  \n",
       "0                                                  NaN                \n",
       "1                                                  NaN                \n",
       "2                                                  NaN                \n",
       "3                                                  NaN                \n",
       "4                                                  NaN                \n",
       "..                                                 ...                \n",
       "138                                                NaN                \n",
       "139                                                NaN                \n",
       "140                                                NaN                \n",
       "141                                                NaN                \n",
       "142                                                NaN                \n",
       "\n",
       "[143 rows x 6 columns]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# a dataframe is a \"spreadsheet in Python\"   (seems to have an extra column!)\n",
    "#\n",
    "pd.set_option('display.max_rows', 10)     # None for no limit; default: 10\n",
    "# pd.set_option('display.min_rows', 150)   # min_rows is not universally supported...\n",
    "# let's view it!\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 143 entries, 0 to 142\n",
      "Data columns (total 6 columns):\n",
      " #   Column                                                           Non-Null Count  Dtype  \n",
      "---  ------                                                           --------------  -----  \n",
      " 0   sepallen                                                         143 non-null    float64\n",
      " 1   sepalwid                                                         143 non-null    float64\n",
      " 2   petallen                                                         143 non-null    float64\n",
      " 3   petalwid                                                         143 non-null    float64\n",
      " 4   irisname                                                         142 non-null    object \n",
      " 5   adapted from https://en.wikipedia.org/wiki/Iris_flower_data_set  0 non-null      float64\n",
      "dtypes: float64(5), object(1)\n",
      "memory usage: 6.8+ KB\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# let's look at our pandas dataframe   (Aargh: that extra column!)\n",
    "#\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sepallen', 'sepalwid', 'petallen', 'petalwid', 'irisname',\n",
       "       'adapted from https://en.wikipedia.org/wiki/Iris_flower_data_set'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 143 entries, 0 to 142\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   sepallen  143 non-null    float64\n",
      " 1   sepalwid  143 non-null    float64\n",
      " 2   petallen  143 non-null    float64\n",
      " 3   petalwid  143 non-null    float64\n",
      " 4   irisname  142 non-null    object \n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 5.7+ KB\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# let's drop that last column (dropping is usually by _name_):\n",
    "#\n",
    "#   if you want a list of the column names use df.columns\n",
    "col5name = df.columns[5]  # get column name at index 5\n",
    "df_clean = df.drop(columns=[col5name])  # drop by name is typical\n",
    "df_clean.info()                         # should be happier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLUMNS is Index(['sepallen', 'sepalwid', 'petallen', 'petalwid', 'irisname'], dtype='object')\n",
      "\n",
      "COLUMNS[0] is sepallen\n",
      "\n",
      "COL_INDEX is {'sepallen': 0, 'sepalwid': 1, 'petallen': 2, 'petalwid': 3, 'irisname': 4}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# let's keep our column names in variables, for reference\n",
    "#\n",
    "COLUMNS = df_clean.columns            # \"list\" of columns\n",
    "print(f\"COLUMNS is {COLUMNS}\\n\")  \n",
    "  # It's a \"pandas\" list, called an Index\n",
    "  # use it just as a Python list of strings:\n",
    "print(f\"COLUMNS[0] is {COLUMNS[0]}\\n\")\n",
    "\n",
    "# let's create a dictionary to look up any column index by name\n",
    "COL_INDEX = {}\n",
    "for i, name in enumerate(COLUMNS):\n",
    "    COL_INDEX[name] = i  # using the name (as key), look up the value (i)\n",
    "print(f\"COL_INDEX is {COL_INDEX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 143 entries, 0 to 142\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   sepallen  143 non-null    float64\n",
      " 1   sepalwid  143 non-null    float64\n",
      " 2   petallen  143 non-null    float64\n",
      " 3   petalwid  143 non-null    float64\n",
      " 4   irisname  142 non-null    object \n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 5.7+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepallen</th>\n",
       "      <th>sepalwid</th>\n",
       "      <th>petallen</th>\n",
       "      <th>petalwid</th>\n",
       "      <th>irisname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000e-01</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.000000e-01</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.000000e-01</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>2.000000e-01</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.200000e+00</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.300000e+00</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>4.2</td>\n",
       "      <td>42.0</td>\n",
       "      <td>4242.0</td>\n",
       "      <td>4.200000e+42</td>\n",
       "      <td>alieniris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>4.2</td>\n",
       "      <td>42.0</td>\n",
       "      <td>4242.0</td>\n",
       "      <td>4.200000e+42</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepallen  sepalwid  petallen      petalwid   irisname\n",
       "0         4.6       3.6       1.0  2.000000e-01     setosa\n",
       "1         4.3       3.0       1.1  1.000000e-01     setosa\n",
       "2         5.0       3.2       1.2  2.000000e-01     setosa\n",
       "3         5.8       4.0       1.2  2.000000e-01     setosa\n",
       "4         4.4       3.0       1.3  2.000000e-01     setosa\n",
       "..        ...       ...       ...           ...        ...\n",
       "138       7.7       3.8       6.7  2.200000e+00  virginica\n",
       "139       7.7       2.8       6.7  2.000000e+00  virginica\n",
       "140       7.7       2.6       6.9  2.300000e+00  virginica\n",
       "141       4.2      42.0    4242.0  4.200000e+42  alieniris\n",
       "142       4.2      42.0    4242.0  4.200000e+42        NaN\n",
       "\n",
       "[143 rows x 5 columns]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# let's look at our cleaned-up dataframe...\n",
    "#\n",
    "df_clean.info()   \n",
    "#\n",
    "# notice that the non-null is _different_ for irisname!\n",
    "df_clean   # show a table! (the problem rows are the last two...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 142 entries, 0 to 141\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   sepallen  142 non-null    float64\n",
      " 1   sepalwid  142 non-null    float64\n",
      " 2   petallen  142 non-null    float64\n",
      " 3   petalwid  142 non-null    float64\n",
      " 4   irisname  142 non-null    object \n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 6.7+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepallen</th>\n",
       "      <th>sepalwid</th>\n",
       "      <th>petallen</th>\n",
       "      <th>petalwid</th>\n",
       "      <th>irisname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000e-01</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.000000e-01</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.000000e-01</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>2.000000e-01</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>7.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.100000e+00</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.200000e+00</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.300000e+00</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>4.2</td>\n",
       "      <td>42.0</td>\n",
       "      <td>4242.0</td>\n",
       "      <td>4.200000e+42</td>\n",
       "      <td>alieniris</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepallen  sepalwid  petallen      petalwid   irisname\n",
       "0         4.6       3.6       1.0  2.000000e-01     setosa\n",
       "1         4.3       3.0       1.1  1.000000e-01     setosa\n",
       "2         5.0       3.2       1.2  2.000000e-01     setosa\n",
       "3         5.8       4.0       1.2  2.000000e-01     setosa\n",
       "4         4.4       3.0       1.3  2.000000e-01     setosa\n",
       "..        ...       ...       ...           ...        ...\n",
       "137       7.6       3.0       6.6  2.100000e+00  virginica\n",
       "138       7.7       3.8       6.7  2.200000e+00  virginica\n",
       "139       7.7       2.8       6.7  2.000000e+00  virginica\n",
       "140       7.7       2.6       6.9  2.300000e+00  virginica\n",
       "141       4.2      42.0    4242.0  4.200000e+42  alieniris\n",
       "\n",
       "[142 rows x 5 columns]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# typically, after dropping columns we don't want, \n",
    "#   we drop rows with missing data (other approaches are possible, too)\n",
    "#\n",
    "df_full = df_clean.dropna()   # this removes all rows with missing data (\"na\")\n",
    "df_full.info()                # it's \"full\" because it has no missing data\n",
    "df_full\n",
    "#\n",
    "# notice that _all_ of the rows now have 142 non-null items\n",
    "#    also, the last row isn't real data... we'll handle it next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(141, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepallen</th>\n",
       "      <th>sepalwid</th>\n",
       "      <th>petallen</th>\n",
       "      <th>petalwid</th>\n",
       "      <th>irisname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>7.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.2</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepallen  sepalwid  petallen  petalwid   irisname\n",
       "0         4.6       3.6       1.0       0.2     setosa\n",
       "1         4.3       3.0       1.1       0.1     setosa\n",
       "2         5.0       3.2       1.2       0.2     setosa\n",
       "3         5.8       4.0       1.2       0.2     setosa\n",
       "4         4.4       3.0       1.3       0.2     setosa\n",
       "..        ...       ...       ...       ...        ...\n",
       "136       7.9       3.8       6.4       2.0  virginica\n",
       "137       7.6       3.0       6.6       2.1  virginica\n",
       "138       7.7       3.8       6.7       2.2  virginica\n",
       "139       7.7       2.8       6.7       2.0  virginica\n",
       "140       7.7       2.6       6.9       2.3  virginica\n",
       "\n",
       "[141 rows x 5 columns]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "# get rid of last row!\n",
    "#\n",
    "df_final = df_full.iloc[0:-1]     # not the syntax I would choose\n",
    "print(df_final.shape)\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setosa maps to 0\n",
      "versicolor maps to 1\n",
      "virginica maps to 2\n"
     ]
    }
   ],
   "source": [
    "# all of scikit-learn's ML routines need numbers, not strings\n",
    "#   ... even for categories/classifications (like species!)\n",
    "#   so, we will convert the flower-species to numbers:\n",
    "\n",
    "SPECIES = ['setosa','versicolor','virginica']   # int to str\n",
    "SPECIES_INDEX = {'setosa':0,'versicolor':1,'virginica':2}  # str to int\n",
    "\n",
    "def convert_species(speciesname):\n",
    "    \"\"\" return the species index (a unique integer/category) \"\"\"\n",
    "    #print(f\"converting {speciesname}...\")\n",
    "    return SPECIES_INDEX[speciesname]\n",
    "\n",
    "# Let's try it out...\n",
    "for name in SPECIES:\n",
    "    print(f\"{name} maps to {convert_species(name)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chiachinghsieh/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# we can \"apply\" a function to a whole column\n",
    "#   it may give a warning; here, this is ok ...\n",
    "#\n",
    "\n",
    "df_final['irisname'] = df_final['irisname'].apply(convert_species)\n",
    "\n",
    "# Don't run this twice: the data will be different the second time!\n",
    "#   (In reality, feel free to go back and re-run cells to re-establish things... :-)\n",
    "#    Don't worry about the (possible)  \"SettingWithCopyWarning\" here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepallen</th>\n",
       "      <th>sepalwid</th>\n",
       "      <th>petallen</th>\n",
       "      <th>petalwid</th>\n",
       "      <th>irisname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>7.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepallen  sepalwid  petallen  petalwid  irisname\n",
       "0         4.6       3.6       1.0       0.2         0\n",
       "1         4.3       3.0       1.1       0.1         0\n",
       "2         5.0       3.2       1.2       0.2         0\n",
       "3         5.8       4.0       1.2       0.2         0\n",
       "4         4.4       3.0       1.3       0.2         0\n",
       "..        ...       ...       ...       ...       ...\n",
       "136       7.9       3.8       6.4       2.0         2\n",
       "137       7.6       3.0       6.6       2.1         2\n",
       "138       7.7       3.8       6.7       2.2         2\n",
       "139       7.7       2.8       6.7       2.0         2\n",
       "140       7.7       2.6       6.9       2.3         2\n",
       "\n",
       "[141 rows x 5 columns]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# let's see it!  (this is safe to run many times...)\n",
    "#\n",
    "df_final         # print(df_final.to_string())  # for _all_ rows..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.6 3.6 1.  0.2 0. ]\n",
      " [4.3 3.  1.1 0.1 0. ]\n",
      " [5.  3.2 1.2 0.2 0. ]\n",
      " [5.8 4.  1.2 0.2 0. ]\n",
      " [4.4 3.  1.3 0.2 0. ]\n",
      " [4.4 3.2 1.3 0.2 0. ]\n",
      " [4.5 2.3 1.3 0.3 0. ]\n",
      " [4.7 3.2 1.3 0.2 0. ]\n",
      " [5.  3.5 1.3 0.3 0. ]\n",
      " [5.4 3.9 1.3 0.4 0. ]\n",
      " [5.5 3.5 1.3 0.2 0. ]\n",
      " [4.4 2.9 1.4 0.2 0. ]\n",
      " [4.6 3.4 1.4 0.3 0. ]\n",
      " [4.6 3.2 1.4 0.2 0. ]\n",
      " [4.8 3.  1.4 0.1 0. ]\n",
      " [4.8 3.  1.4 0.3 0. ]\n",
      " [4.9 3.  1.4 0.2 0. ]\n",
      " [5.  3.6 1.4 0.2 0. ]\n",
      " [5.  3.3 1.4 0.2 0. ]\n",
      " [5.1 3.5 1.4 0.2 0. ]\n",
      " [5.1 3.5 1.4 0.3 0. ]\n",
      " [5.2 3.4 1.4 0.2 0. ]\n",
      " [5.5 4.2 1.4 0.2 0. ]\n",
      " [4.6 3.1 1.5 0.2 0. ]\n",
      " [4.9 3.1 1.5 0.1 0. ]\n",
      " [4.9 3.1 1.5 0.1 0. ]\n",
      " [4.9 3.1 1.5 0.1 0. ]\n",
      " [5.  3.4 1.5 0.2 0. ]\n",
      " [5.1 3.8 1.5 0.3 0. ]\n",
      " [5.1 3.7 1.5 0.4 0. ]\n",
      " [5.1 3.4 1.5 0.2 0. ]\n",
      " [5.2 3.5 1.5 0.2 0. ]\n",
      " [5.3 3.7 1.5 0.2 0. ]\n",
      " [5.4 3.7 1.5 0.2 0. ]\n",
      " [5.7 4.4 1.5 0.4 0. ]\n",
      " [4.7 3.2 1.6 0.2 0. ]\n",
      " [4.8 3.4 1.6 0.2 0. ]\n",
      " [5.  3.  1.6 0.2 0. ]\n",
      " [5.  3.4 1.6 0.4 0. ]\n",
      " [5.  3.5 1.6 0.6 0. ]\n",
      " [5.1 3.8 1.6 0.2 0. ]\n",
      " [5.1 3.3 1.7 0.5 0. ]\n",
      " [5.4 3.9 1.7 0.4 0. ]\n",
      " [5.4 3.4 1.7 0.2 0. ]\n",
      " [5.7 3.8 1.7 0.3 0. ]\n",
      " [4.8 3.4 1.9 0.2 0. ]\n",
      " [5.1 3.8 1.9 0.4 0. ]\n",
      " [7.  3.2 4.7 1.4 1. ]\n",
      " [4.9 2.4 3.3 1.  1. ]\n",
      " [5.  2.3 3.3 1.  1. ]\n",
      " [5.  2.  3.5 1.  1. ]\n",
      " [5.7 2.6 3.5 1.  1. ]\n",
      " [5.6 2.9 3.6 1.3 1. ]\n",
      " [5.5 2.4 3.7 1.  1. ]\n",
      " [5.5 2.4 3.8 1.1 1. ]\n",
      " [5.2 2.7 3.9 1.4 1. ]\n",
      " [5.6 2.5 3.9 1.1 1. ]\n",
      " [5.8 2.7 3.9 1.2 1. ]\n",
      " [5.5 2.3 4.  1.3 1. ]\n",
      " [5.5 2.5 4.  1.3 1. ]\n",
      " [5.8 2.6 4.  1.2 1. ]\n",
      " [6.  2.2 4.  1.  1. ]\n",
      " [6.1 2.8 4.  1.3 1. ]\n",
      " [5.6 3.  4.1 1.3 1. ]\n",
      " [5.8 2.7 4.1 1.  1. ]\n",
      " [5.6 2.7 4.2 1.3 1. ]\n",
      " [5.7 3.  4.2 1.2 1. ]\n",
      " [5.9 3.  4.2 1.5 1. ]\n",
      " [6.4 2.9 4.3 1.3 1. ]\n",
      " [5.5 2.6 4.4 1.2 1. ]\n",
      " [6.3 2.3 4.4 1.3 1. ]\n",
      " [6.6 3.  4.4 1.4 1. ]\n",
      " [6.7 3.1 4.4 1.4 1. ]\n",
      " [5.4 3.  4.5 1.5 1. ]\n",
      " [5.6 3.  4.5 1.5 1. ]\n",
      " [5.7 2.8 4.5 1.3 1. ]\n",
      " [6.  2.9 4.5 1.5 1. ]\n",
      " [6.  3.4 4.5 1.6 1. ]\n",
      " [6.2 2.2 4.5 1.5 1. ]\n",
      " [6.4 3.2 4.5 1.5 1. ]\n",
      " [6.1 3.  4.6 1.4 1. ]\n",
      " [6.5 2.8 4.6 1.5 1. ]\n",
      " [6.6 2.9 4.6 1.3 1. ]\n",
      " [6.1 2.9 4.7 1.4 1. ]\n",
      " [6.1 2.8 4.7 1.2 1. ]\n",
      " [6.3 3.3 4.7 1.6 1. ]\n",
      " [6.7 3.1 4.7 1.5 1. ]\n",
      " [5.9 3.2 4.8 1.8 1. ]\n",
      " [6.8 2.8 4.8 1.4 1. ]\n",
      " [6.3 2.5 4.9 1.5 1. ]\n",
      " [6.9 3.1 4.9 1.5 1. ]\n",
      " [6.7 3.  5.  1.7 1. ]\n",
      " [6.  2.7 5.1 1.6 1. ]\n",
      " [4.9 2.5 4.5 1.7 2. ]\n",
      " [6.  3.  4.8 1.8 2. ]\n",
      " [6.2 2.8 4.8 1.8 2. ]\n",
      " [5.6 2.8 4.9 2.  2. ]\n",
      " [6.1 3.  4.9 1.8 2. ]\n",
      " [6.3 2.7 4.9 1.8 2. ]\n",
      " [5.7 2.5 5.  2.  2. ]\n",
      " [6.  2.2 5.  1.5 2. ]\n",
      " [6.3 2.5 5.  1.9 2. ]\n",
      " [5.8 2.8 5.1 2.4 2. ]\n",
      " [5.8 2.7 5.1 1.9 2. ]\n",
      " [5.9 3.  5.1 1.8 2. ]\n",
      " [6.3 2.8 5.1 1.5 2. ]\n",
      " [6.5 3.2 5.1 2.  2. ]\n",
      " [6.9 3.1 5.1 2.3 2. ]\n",
      " [6.5 3.  5.2 2.  2. ]\n",
      " [6.7 3.  5.2 2.3 2. ]\n",
      " [6.4 2.7 5.3 1.9 2. ]\n",
      " [6.4 3.2 5.3 2.3 2. ]\n",
      " [6.2 3.4 5.4 2.3 2. ]\n",
      " [6.9 3.1 5.4 2.1 2. ]\n",
      " [6.4 3.1 5.5 1.8 2. ]\n",
      " [6.5 3.  5.5 1.8 2. ]\n",
      " [6.8 3.  5.5 2.1 2. ]\n",
      " [6.1 2.6 5.6 1.4 2. ]\n",
      " [6.3 2.9 5.6 1.8 2. ]\n",
      " [6.3 3.4 5.6 2.4 2. ]\n",
      " [6.4 2.8 5.6 2.1 2. ]\n",
      " [6.4 2.8 5.6 2.2 2. ]\n",
      " [6.7 3.1 5.6 2.4 2. ]\n",
      " [6.7 3.3 5.7 2.1 2. ]\n",
      " [6.7 3.3 5.7 2.5 2. ]\n",
      " [6.9 3.2 5.7 2.3 2. ]\n",
      " [6.5 3.  5.8 2.2 2. ]\n",
      " [6.7 2.5 5.8 1.8 2. ]\n",
      " [7.2 3.  5.8 1.6 2. ]\n",
      " [6.8 3.2 5.9 2.3 2. ]\n",
      " [7.1 3.  5.9 2.1 2. ]\n",
      " [7.2 3.2 6.  1.8 2. ]\n",
      " [7.2 3.6 6.1 2.5 2. ]\n",
      " [7.4 2.8 6.1 1.9 2. ]\n",
      " [7.7 3.  6.1 2.3 2. ]\n",
      " [7.3 2.9 6.3 1.8 2. ]\n",
      " [7.9 3.8 6.4 2.  2. ]\n",
      " [7.6 3.  6.6 2.1 2. ]\n",
      " [7.7 3.8 6.7 2.2 2. ]\n",
      " [7.7 2.8 6.7 2.  2. ]\n",
      " [7.7 2.6 6.9 2.3 2. ]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# let's convert our dataframe to a numpy array, named A\n",
    "#    Our ML library, scikit-learn operates entirely on numpy arrays.\n",
    "#\n",
    "A = df_final.values    # .values gets the numpy array\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.6 3.6 1.  0.2 0. ]\n",
      " [4.3 3.  1.1 0.1 0. ]\n",
      " [5.  3.2 1.2 0.2 0. ]\n",
      " [5.8 4.  1.2 0.2 0. ]\n",
      " [4.4 3.  1.3 0.2 0. ]\n",
      " [4.4 3.2 1.3 0.2 0. ]\n",
      " [4.5 2.3 1.3 0.3 0. ]\n",
      " [4.7 3.2 1.3 0.2 0. ]\n",
      " [5.  3.5 1.3 0.3 0. ]\n",
      " [5.4 3.9 1.3 0.4 0. ]\n",
      " [5.5 3.5 1.3 0.2 0. ]\n",
      " [4.4 2.9 1.4 0.2 0. ]\n",
      " [4.6 3.4 1.4 0.3 0. ]\n",
      " [4.6 3.2 1.4 0.2 0. ]\n",
      " [4.8 3.  1.4 0.1 0. ]\n",
      " [4.8 3.  1.4 0.3 0. ]\n",
      " [4.9 3.  1.4 0.2 0. ]\n",
      " [5.  3.6 1.4 0.2 0. ]\n",
      " [5.  3.3 1.4 0.2 0. ]\n",
      " [5.1 3.5 1.4 0.2 0. ]\n",
      " [5.1 3.5 1.4 0.3 0. ]\n",
      " [5.2 3.4 1.4 0.2 0. ]\n",
      " [5.5 4.2 1.4 0.2 0. ]\n",
      " [4.6 3.1 1.5 0.2 0. ]\n",
      " [4.9 3.1 1.5 0.1 0. ]\n",
      " [4.9 3.1 1.5 0.1 0. ]\n",
      " [4.9 3.1 1.5 0.1 0. ]\n",
      " [5.  3.4 1.5 0.2 0. ]\n",
      " [5.1 3.8 1.5 0.3 0. ]\n",
      " [5.1 3.7 1.5 0.4 0. ]\n",
      " [5.1 3.4 1.5 0.2 0. ]\n",
      " [5.2 3.5 1.5 0.2 0. ]\n",
      " [5.3 3.7 1.5 0.2 0. ]\n",
      " [5.4 3.7 1.5 0.2 0. ]\n",
      " [5.7 4.4 1.5 0.4 0. ]\n",
      " [4.7 3.2 1.6 0.2 0. ]\n",
      " [4.8 3.4 1.6 0.2 0. ]\n",
      " [5.  3.  1.6 0.2 0. ]\n",
      " [5.  3.4 1.6 0.4 0. ]\n",
      " [5.  3.5 1.6 0.6 0. ]\n",
      " [5.1 3.8 1.6 0.2 0. ]\n",
      " [5.1 3.3 1.7 0.5 0. ]\n",
      " [5.4 3.9 1.7 0.4 0. ]\n",
      " [5.4 3.4 1.7 0.2 0. ]\n",
      " [5.7 3.8 1.7 0.3 0. ]\n",
      " [4.8 3.4 1.9 0.2 0. ]\n",
      " [5.1 3.8 1.9 0.4 0. ]\n",
      " [7.  3.2 4.7 1.4 1. ]\n",
      " [4.9 2.4 3.3 1.  1. ]\n",
      " [5.  2.3 3.3 1.  1. ]\n",
      " [5.  2.  3.5 1.  1. ]\n",
      " [5.7 2.6 3.5 1.  1. ]\n",
      " [5.6 2.9 3.6 1.3 1. ]\n",
      " [5.5 2.4 3.7 1.  1. ]\n",
      " [5.5 2.4 3.8 1.1 1. ]\n",
      " [5.2 2.7 3.9 1.4 1. ]\n",
      " [5.6 2.5 3.9 1.1 1. ]\n",
      " [5.8 2.7 3.9 1.2 1. ]\n",
      " [5.5 2.3 4.  1.3 1. ]\n",
      " [5.5 2.5 4.  1.3 1. ]\n",
      " [5.8 2.6 4.  1.2 1. ]\n",
      " [6.  2.2 4.  1.  1. ]\n",
      " [6.1 2.8 4.  1.3 1. ]\n",
      " [5.6 3.  4.1 1.3 1. ]\n",
      " [5.8 2.7 4.1 1.  1. ]\n",
      " [5.6 2.7 4.2 1.3 1. ]\n",
      " [5.7 3.  4.2 1.2 1. ]\n",
      " [5.9 3.  4.2 1.5 1. ]\n",
      " [6.4 2.9 4.3 1.3 1. ]\n",
      " [5.5 2.6 4.4 1.2 1. ]\n",
      " [6.3 2.3 4.4 1.3 1. ]\n",
      " [6.6 3.  4.4 1.4 1. ]\n",
      " [6.7 3.1 4.4 1.4 1. ]\n",
      " [5.4 3.  4.5 1.5 1. ]\n",
      " [5.6 3.  4.5 1.5 1. ]\n",
      " [5.7 2.8 4.5 1.3 1. ]\n",
      " [6.  2.9 4.5 1.5 1. ]\n",
      " [6.  3.4 4.5 1.6 1. ]\n",
      " [6.2 2.2 4.5 1.5 1. ]\n",
      " [6.4 3.2 4.5 1.5 1. ]\n",
      " [6.1 3.  4.6 1.4 1. ]\n",
      " [6.5 2.8 4.6 1.5 1. ]\n",
      " [6.6 2.9 4.6 1.3 1. ]\n",
      " [6.1 2.9 4.7 1.4 1. ]\n",
      " [6.1 2.8 4.7 1.2 1. ]\n",
      " [6.3 3.3 4.7 1.6 1. ]\n",
      " [6.7 3.1 4.7 1.5 1. ]\n",
      " [5.9 3.2 4.8 1.8 1. ]\n",
      " [6.8 2.8 4.8 1.4 1. ]\n",
      " [6.3 2.5 4.9 1.5 1. ]\n",
      " [6.9 3.1 4.9 1.5 1. ]\n",
      " [6.7 3.  5.  1.7 1. ]\n",
      " [6.  2.7 5.1 1.6 1. ]\n",
      " [4.9 2.5 4.5 1.7 2. ]\n",
      " [6.  3.  4.8 1.8 2. ]\n",
      " [6.2 2.8 4.8 1.8 2. ]\n",
      " [5.6 2.8 4.9 2.  2. ]\n",
      " [6.1 3.  4.9 1.8 2. ]\n",
      " [6.3 2.7 4.9 1.8 2. ]\n",
      " [5.7 2.5 5.  2.  2. ]\n",
      " [6.  2.2 5.  1.5 2. ]\n",
      " [6.3 2.5 5.  1.9 2. ]\n",
      " [5.8 2.8 5.1 2.4 2. ]\n",
      " [5.8 2.7 5.1 1.9 2. ]\n",
      " [5.9 3.  5.1 1.8 2. ]\n",
      " [6.3 2.8 5.1 1.5 2. ]\n",
      " [6.5 3.2 5.1 2.  2. ]\n",
      " [6.9 3.1 5.1 2.3 2. ]\n",
      " [6.5 3.  5.2 2.  2. ]\n",
      " [6.7 3.  5.2 2.3 2. ]\n",
      " [6.4 2.7 5.3 1.9 2. ]\n",
      " [6.4 3.2 5.3 2.3 2. ]\n",
      " [6.2 3.4 5.4 2.3 2. ]\n",
      " [6.9 3.1 5.4 2.1 2. ]\n",
      " [6.4 3.1 5.5 1.8 2. ]\n",
      " [6.5 3.  5.5 1.8 2. ]\n",
      " [6.8 3.  5.5 2.1 2. ]\n",
      " [6.1 2.6 5.6 1.4 2. ]\n",
      " [6.3 2.9 5.6 1.8 2. ]\n",
      " [6.3 3.4 5.6 2.4 2. ]\n",
      " [6.4 2.8 5.6 2.1 2. ]\n",
      " [6.4 2.8 5.6 2.2 2. ]\n",
      " [6.7 3.1 5.6 2.4 2. ]\n",
      " [6.7 3.3 5.7 2.1 2. ]\n",
      " [6.7 3.3 5.7 2.5 2. ]\n",
      " [6.9 3.2 5.7 2.3 2. ]\n",
      " [6.5 3.  5.8 2.2 2. ]\n",
      " [6.7 2.5 5.8 1.8 2. ]\n",
      " [7.2 3.  5.8 1.6 2. ]\n",
      " [6.8 3.2 5.9 2.3 2. ]\n",
      " [7.1 3.  5.9 2.1 2. ]\n",
      " [7.2 3.2 6.  1.8 2. ]\n",
      " [7.2 3.6 6.1 2.5 2. ]\n",
      " [7.4 2.8 6.1 1.9 2. ]\n",
      " [7.7 3.  6.1 2.3 2. ]\n",
      " [7.3 2.9 6.3 1.8 2. ]\n",
      " [7.9 3.8 6.4 2.  2. ]\n",
      " [7.6 3.  6.6 2.1 2. ]\n",
      " [7.7 3.8 6.7 2.2 2. ]\n",
      " [7.7 2.8 6.7 2.  2. ]\n",
      " [7.7 2.6 6.9 2.3 2. ]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# let's make sure it's all floating-point, so we can multiply and divide\n",
    "#\n",
    "A = A.astype('float64')  # so many:  www.tutorialspoint.com/numpy/numpy_data_types.htm\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The dataset has 141 rows and 5 cols\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# nice to have NUM_ROWS and NUM_COLS around\n",
    "#\n",
    "NUM_ROWS, NUM_COLS = A.shape\n",
    "print(f\"\\nThe dataset has {NUM_ROWS} rows and {NUM_COLS} cols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flower #132 is [7.2 3.6 6.1 2.5 2. ]\n",
      "  Its sepallen is 7.2\n",
      "  Its sepalwid is 3.6\n",
      "  Its petallen is 6.1\n",
      "  Its petalwid is 2.5\n",
      "  Its irisname is virginica (2)\n"
     ]
    }
   ],
   "source": [
    "# let's use all of our variables, to reinforce names...\n",
    "\n",
    "# choose a row index, n:\n",
    "n = 132\n",
    "print(f\"flower #{n} is {A[n]}\")\n",
    "\n",
    "for i in range(len(COLUMNS)):\n",
    "    colname = COLUMNS[i]\n",
    "    if colname != 'irisname':\n",
    "        print(f\"  Its {colname} is {A[n][i]}\")\n",
    "    else:\n",
    "        species_num = int(A[n][i])\n",
    "        species = SPECIES[species_num]\n",
    "        print(f\"  Its {colname} is {species} ({species_num})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# we could write-our-own, but we don't have to! Let's \"library\"! After all,\n",
    "#\n",
    "#     the representation and storage for the trees is a big task\n",
    "#     we want an already-debugged algorithm!\n",
    "#     we want to ask q'ns about irises and how \"classifiable\" they are, \n",
    "#        rather than questions about implementation (at least for this moment...)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Start of data definitions +++\n",
      "\n",
      "X_all (just features) is \n",
      " [[4.6 3.6 1.  0.2]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [5.1 3.5 1.4 0.2]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [7.7 2.6 6.9 2.3]]\n",
      "y_all (just labels)   is \n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      "Weighting sepallen by 1.0\n",
      "Weighting sepalwid by 1.0\n",
      "Weighting petallen by 1.0\n",
      "Weighting petalwid by 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"+++ Start of data definitions +++\\n\")\n",
    "\n",
    "X_all = A[:,0:4].copy()  # X (features) ... is all rows, columns 0, 1, 2, 3\n",
    "y_all = A[:,4].copy()    # y (labels) ... is all rows, column 4 only\n",
    "\n",
    "print(f\"X_all (just features) is \\n {X_all}\")\n",
    "print(f\"y_all (just labels)   is \\n {y_all}\") \n",
    "\n",
    "\n",
    "# we can re-weight different features here...\n",
    "COL_WEIGHT = {              # could be called Feature weight...\n",
    "    'sepallen':1.0,\n",
    "    'sepalwid':1.0,\n",
    "    'petallen':1.0,\n",
    "    'petalwid':1.0,\n",
    "}\n",
    "\n",
    "for colname in COL_WEIGHT:\n",
    "    i = COL_INDEX[colname]    # get the column index, i, of the colname\n",
    "    weight = COL_WEIGHT[colname]  # from the dictionary above\n",
    "    print(\"Weighting\", colname, \"by\", weight)   \n",
    "    # weighting == \"multiplying\"\n",
    "    X_all[:,i] *= weight   # multiply by the weight to give this column (\"feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.9 3.1 4.9 1.5]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [6.7 3.  5.  1.7]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [5.  3.  1.6 0.2]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.1 3.5 1.4 0.2]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [5.8 2.6 4.  1.2]]\n",
      "[1. 2. 2. 0. 1. 2. 1. 1. 1. 0. 2. 0. 1. 2. 1. 2. 2. 2. 2. 1. 0. 0. 1. 1.\n",
      " 0. 1. 0. 1. 2. 0. 2. 2. 0. 2. 1. 0. 1. 1. 2. 2. 0. 2. 2. 2. 0. 0. 0. 1.\n",
      " 1. 2. 0. 0. 0. 0. 0. 1. 1. 2. 1. 1. 2. 1. 0. 0. 2. 0. 2. 1. 2. 1. 0. 2.\n",
      " 2. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 2. 2. 1. 0. 2. 2. 1. 2. 2. 2. 1.\n",
      " 0. 0. 0. 1. 2. 0. 2. 0. 0. 2. 2. 1. 2. 1. 1. 2. 2. 1. 1. 0. 0. 1. 2. 0.\n",
      " 2. 2. 0. 2. 0. 2. 2. 0. 1. 0. 1. 1. 0. 1. 0. 2. 0. 0. 2. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# we scramble the data, to give a different TRAIN/TEST split each time...\n",
    "# \n",
    "indices = np.random.permutation(len(y_all))  # indices is a permutation-list\n",
    "\n",
    "# we scramble both X and y, necessarily with the same permutation\n",
    "X_labeled = X_all[indices]              # we apply the _same_ permutation to each!\n",
    "y_labeled = y_all[indices]              # again...\n",
    "print(X_labeled)\n",
    "print(y_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with 113 rows;  testing with 28 rows\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# We next separate into test data and training data ... \n",
    "#    + We will train on the training data...\n",
    "#    + We will _not_ look at the testing data to build the model\n",
    "#\n",
    "# Then, afterward, we will test on the testing data -- and see how well we do!\n",
    "#\n",
    "\n",
    "#\n",
    "# a common convention:  train on 80%, test on 20%    Let's define the TEST_PERCENT\n",
    "#\n",
    "NUM_ROWS = X_labeled.shape[0]     # the number of labeled rows\n",
    "TEST_PERCENT = 0.20\n",
    "TEST_SIZE = int(TEST_PERCENT*NUM_ROWS)   # no harm in rounding down\n",
    "\n",
    "X_test = X_labeled[:TEST_SIZE]    # first section are for testing\n",
    "y_test = y_labeled[:TEST_SIZE]\n",
    "\n",
    "X_train = X_labeled[TEST_SIZE:]   # all the rest are for training\n",
    "y_train = y_labeled[TEST_SIZE:]\n",
    "\n",
    "print(f\"training with {len(y_train)} rows;  testing with {len(y_test)} rows\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test is [1. 2. 2. 0. 1. 2. 1. 1. 1. 0. 2. 0. 1. 2. 1. 2. 2. 2. 2. 1. 0. 0. 1. 1.\n",
      " 0. 1. 0. 1.]\n",
      "y_train is [2. 0. 2. 2. 0. 2. 1. 0. 1. 1. 2. 2. 0. 2. 2. 2. 0. 0. 0. 1. 1. 2. 0. 0.\n",
      " 0. 0. 0. 1. 1. 2. 1. 1. 2. 1. 0. 0. 2. 0. 2. 1. 2. 1. 0. 2. 2. 1. 1. 0.\n",
      " 0. 1. 0. 1. 0. 1. 0. 1. 1. 2. 2. 1. 0. 2. 2. 1. 2. 2. 2. 1. 0. 0. 0. 1.\n",
      " 2. 0. 2. 0. 0. 2. 2. 1. 2. 1. 1. 2. 2. 1. 1. 0. 0. 1. 2. 0. 2. 2. 0. 2.\n",
      " 0. 2. 2. 0. 1. 0. 1. 1. 0. 1. 0. 2. 0. 0. 2. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(f\"y_test is {y_test}\")\n",
    "print(f\"y_train is {y_train}\")   # to \"get a visual\" on these...\n",
    "# print(X_test)\n",
    "# print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created and trained a knn classifier with k = 10\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# +++ This is a \"Model-building and Model-training Cell\"\n",
    "#       \n",
    "# Create a kNN model and train it! \n",
    "#\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "k = 10   # we don't know what k to use, so we guess!  (this will _not_ be a good value)\n",
    "knn_model = KNeighborsClassifier(n_neighbors=k)       # here, k is the \"k\" in kNN\n",
    "\n",
    "# we train the model (it's one line!)\n",
    "knn_model.fit(X_train, y_train)                              # yay!  trained!\n",
    "print(\"Created and trained a knn classifier with k =\", k)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels: [1. 2. 2. 0. 1. 2. 1. 1. 1. 0. 2. 0. 1. 2. 1. 2. 2. 2. 2. 1. 0. 0. 1. 1.\n",
      " 0. 1. 0. 1.]\n",
      "Actual  labels  : [1. 2. 2. 0. 1. 2. 1. 1. 1. 0. 2. 0. 1. 2. 1. 2. 2. 2. 2. 1. 0. 0. 1. 1.\n",
      " 0. 1. 0. 1.]\n",
      "\n",
      "Results on test set:  28 correct out of 28 total.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# +++ This is the \"Model-testing Cell\"\n",
    "#\n",
    "# Now, let's see how well we did on our \"held-out data\" (the testing data)\n",
    "#\n",
    "\n",
    "# We run our test set!\n",
    "predicted_labels = knn_model.predict(X_test)\n",
    "actual_labels = y_test\n",
    "\n",
    "# Let's print them so we can compare...\n",
    "print(\"Predicted labels:\", predicted_labels)\n",
    "print(\"Actual  labels  :\", actual_labels)\n",
    "\n",
    "# And, some overall results\n",
    "num_correct = sum(predicted_labels == actual_labels)\n",
    "total = len(actual_labels)\n",
    "print(f\"\\nResults on test set:  {num_correct} correct out of {total} total.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row   0 :   versicolor versicolor     \n",
      "row   1 :    virginica virginica      \n",
      "row   2 :    virginica virginica      \n",
      "row   3 :       setosa setosa         \n",
      "row   4 :   versicolor versicolor     \n",
      "row   5 :    virginica virginica      \n",
      "row   6 :   versicolor versicolor     \n",
      "row   7 :   versicolor versicolor     \n",
      "row   8 :   versicolor versicolor     \n",
      "row   9 :       setosa setosa         \n",
      "row  10 :    virginica virginica      \n",
      "row  11 :       setosa setosa         \n",
      "row  12 :   versicolor versicolor     \n",
      "row  13 :    virginica virginica      \n",
      "row  14 :   versicolor versicolor     \n",
      "row  15 :    virginica virginica      \n",
      "row  16 :    virginica virginica      \n",
      "row  17 :    virginica virginica      \n",
      "row  18 :    virginica virginica      \n",
      "row  19 :   versicolor versicolor     \n",
      "row  20 :       setosa setosa         \n",
      "row  21 :       setosa setosa         \n",
      "row  22 :   versicolor versicolor     \n",
      "row  23 :   versicolor versicolor     \n",
      "row  24 :       setosa setosa         \n",
      "row  25 :   versicolor versicolor     \n",
      "row  26 :       setosa setosa         \n",
      "row  27 :   versicolor versicolor     \n",
      "\n",
      "Correct: 28 out of 28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Let's print these more helpfully, in a vertical table\n",
    "#\n",
    "\n",
    "def compare_labels(predicted_labels, actual_labels):\n",
    "    \"\"\" a more neatly formatted comparison \"\"\"\n",
    "    NUM_LABELS = len(predicted_labels)\n",
    "    num_correct = 0\n",
    "    \n",
    "    for i in range(NUM_LABELS):\n",
    "        p = int(round(predicted_labels[i]))         # round protects from fp error \n",
    "        a = int(round(actual_labels[i]))\n",
    "        result = \"incorrect\"\n",
    "        if p == a:  # if they match,\n",
    "            result = \"\"       # no longer incorrect\n",
    "            num_correct += 1  # and we count a match!\n",
    "\n",
    "        print(f\"row {i:>3d} : {SPECIES[p]:>12s} {SPECIES[a]:<12s}   {result}\")   \n",
    "\n",
    "    print()\n",
    "    print(\"Correct:\", num_correct, \"out of\", NUM_LABELS)\n",
    "    return num_correct\n",
    "\n",
    "#\n",
    "# let's try it out!\n",
    "#\n",
    "\n",
    "compare_labels(predicted_labels,actual_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Except, we didn't really explore whether this was the BEST model we could build!\n",
    "#\n",
    "#\n",
    "# We used k = 84  (a neighborhood size of 84 flowers)\n",
    "# In a dataset of only 140ish flowers, with three species, this seems like a bad idea!\n",
    "#\n",
    "# Perhaps we should try ALL the neighborhood sizes in their own TRAIN/TEST split\n",
    "# and see which neighborhood size works the best, for irises, at least...\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++best so fair!!!+++++\n",
      "k:  1  cv accuracy:  0.9478\n",
      "k:  2  cv accuracy:  0.9387\n",
      "k:  3  cv accuracy:  0.9478\n",
      "k:  4  cv accuracy:  0.9478\n",
      "+++++best so fair!!!+++++\n",
      "k:  5  cv accuracy:  0.9565\n",
      "k:  6  cv accuracy:  0.9565\n",
      "+++++best so fair!!!+++++\n",
      "k:  7  cv accuracy:  0.9652\n",
      "k:  8  cv accuracy:  0.9565\n",
      "k:  9  cv accuracy:  0.9565\n",
      "k: 10  cv accuracy:  0.9478\n",
      "k: 11  cv accuracy:  0.9652\n",
      "+++++best so fair!!!+++++\n",
      "k: 12  cv accuracy:  0.9739\n",
      "k: 13  cv accuracy:  0.9739\n",
      "k: 14  cv accuracy:  0.9739\n",
      "k: 15  cv accuracy:  0.9739\n",
      "k: 16  cv accuracy:  0.9474\n",
      "k: 17  cv accuracy:  0.9648\n",
      "k: 18  cv accuracy:  0.9648\n",
      "k: 19  cv accuracy:  0.9561\n",
      "k: 20  cv accuracy:  0.9383\n",
      "k: 21  cv accuracy:  0.9474\n",
      "k: 22  cv accuracy:  0.9474\n",
      "k: 23  cv accuracy:  0.9474\n",
      "k: 24  cv accuracy:  0.9387\n",
      "k: 25  cv accuracy:  0.9474\n",
      "k: 26  cv accuracy:  0.9561\n",
      "k: 27  cv accuracy:  0.9474\n",
      "k: 28  cv accuracy:  0.9561\n",
      "k: 29  cv accuracy:  0.9561\n",
      "k: 30  cv accuracy:  0.9474\n",
      "k: 31  cv accuracy:  0.9387\n",
      "k: 32  cv accuracy:  0.9387\n",
      "k: 33  cv accuracy:  0.9300\n",
      "k: 34  cv accuracy:  0.9032\n",
      "k: 35  cv accuracy:  0.9206\n",
      "k: 36  cv accuracy:  0.9206\n",
      "k: 37  cv accuracy:  0.9206\n",
      "k: 38  cv accuracy:  0.9032\n",
      "k: 39  cv accuracy:  0.9032\n",
      "k: 40  cv accuracy:  0.9032\n",
      "k: 41  cv accuracy:  0.9206\n",
      "k: 42  cv accuracy:  0.9032\n",
      "k: 43  cv accuracy:  0.9119\n",
      "k: 44  cv accuracy:  0.9032\n",
      "k: 45  cv accuracy:  0.9032\n",
      "k: 46  cv accuracy:  0.9032\n",
      "k: 47  cv accuracy:  0.9032\n",
      "k: 48  cv accuracy:  0.9032\n",
      "k: 49  cv accuracy:  0.9032\n",
      "k: 50  cv accuracy:  0.9032\n",
      "k: 51  cv accuracy:  0.8771\n",
      "k: 52  cv accuracy:  0.8771\n",
      "k: 53  cv accuracy:  0.8858\n",
      "k: 54  cv accuracy:  0.8941\n",
      "k: 55  cv accuracy:  0.8593\n",
      "k: 56  cv accuracy:  0.8506\n",
      "k: 57  cv accuracy:  0.8233\n",
      "k: 58  cv accuracy:  0.8233\n",
      "k: 59  cv accuracy:  0.8146\n",
      "k: 60  cv accuracy:  0.7968\n",
      "k: 61  cv accuracy:  0.8059\n",
      "k: 62  cv accuracy:  0.7968\n",
      "k: 63  cv accuracy:  0.7968\n",
      "k: 64  cv accuracy:  0.7968\n",
      "k: 65  cv accuracy:  0.7877\n",
      "k: 66  cv accuracy:  0.7877\n",
      "k: 67  cv accuracy:  0.7877\n",
      "k: 68  cv accuracy:  0.7877\n",
      "k: 69  cv accuracy:  0.7877\n",
      "k: 70  cv accuracy:  0.7877\n",
      "k: 71  cv accuracy:  0.7787\n",
      "k: 72  cv accuracy:  0.7787\n",
      "k: 73  cv accuracy:  0.7787\n",
      "k: 74  cv accuracy:  0.7787\n",
      "k: 75  cv accuracy:  0.7700\n",
      "k: 76  cv accuracy:  0.7609\n",
      "k: 77  cv accuracy:  0.7609\n",
      "k: 78  cv accuracy:  0.7518\n",
      "k: 79  cv accuracy:  0.7431\n",
      "k: 80  cv accuracy:  0.7344\n",
      "k: 81  cv accuracy:  0.6992\n",
      "k: 82  cv accuracy:  0.6992\n",
      "k: 83  cv accuracy:  0.6905\n",
      "k: 84  cv accuracy:  0.6992\n",
      "best_k = 12   yields the highest average cv accuracy.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# to do this, we use \"cross validation\"\n",
    "#\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#\n",
    "# cross-validation splits the training set into two pieces:\n",
    "#   + model-building and model-validation. We'll use \"build\" and \"validate\"\n",
    "#\n",
    "\n",
    "best_k = 0\n",
    "best_acc=0\n",
    "\n",
    "for k in range(1,85):\n",
    "    knn_cv_model = KNeighborsClassifier(n_neighbors=k)   # build knn_model for every k!\n",
    "    cv_scores = cross_val_score( knn_cv_model, X_train, y_train, cv=5 )  # 5 means 80/20 split\n",
    "    #print(cv_scores)  # just to see the five scores... \n",
    "    average_cv_accuracy = cv_scores.mean()  # mean() is numpy's built-in average function \n",
    "    if average_cv_accuracy > best_acc:\n",
    "        best_k = k\n",
    "        best_acc= average_cv_accuracy\n",
    "        print(\"+++++best so fair!!!+++++\")\n",
    "    print(f\"k: {k:2d}  cv accuracy: {average_cv_accuracy:7.4f}\")\n",
    "\n",
    "    \n",
    "    \n",
    "# assign best value of k to best_k\n",
    "#best_k = 84      # at the moment this is incorrect   TO DO for hw4pr1: fix this...\n",
    "# you'll need to use the loop above to find and remember the real best_k\n",
    "\n",
    "print(f\"best_k = {best_k}   yields the highest average cv accuracy.\")  # print the best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created + trained a knn classifier, now tuned with a (best) k of 12\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Now, we re-create and re-run the  \"Model-building and -training Cell\"\n",
    "#\n",
    "# Now, using best_k instead of the original, randomly-guessed value    How does it do?!\n",
    "#\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_model_tuned = KNeighborsClassifier(n_neighbors=best_k)   # here, we use the best_k\n",
    "\n",
    "# we train the model (one line!)\n",
    "knn_model_tuned.fit(X_train, y_train)                              # yay!  trained!\n",
    "print(f\"Created + trained a knn classifier, now tuned with a (best) k of {best_k}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels: [1. 2. 2. 0. 1. 2. 1. 1. 1. 0. 2. 0. 1. 2. 1. 2. 2. 2. 2. 1. 0. 0. 1. 1.\n",
      " 0. 1. 0. 1.]\n",
      "Actual labels: [1. 2. 2. 0. 1. 2. 1. 1. 1. 0. 2. 0. 1. 2. 1. 2. 2. 2. 2. 1. 0. 0. 1. 1.\n",
      " 0. 1. 0. 1.]\n",
      "\n",
      "row   0 :   versicolor versicolor     \n",
      "row   1 :    virginica virginica      \n",
      "row   2 :    virginica virginica      \n",
      "row   3 :       setosa setosa         \n",
      "row   4 :   versicolor versicolor     \n",
      "row   5 :    virginica virginica      \n",
      "row   6 :   versicolor versicolor     \n",
      "row   7 :   versicolor versicolor     \n",
      "row   8 :   versicolor versicolor     \n",
      "row   9 :       setosa setosa         \n",
      "row  10 :    virginica virginica      \n",
      "row  11 :       setosa setosa         \n",
      "row  12 :   versicolor versicolor     \n",
      "row  13 :    virginica virginica      \n",
      "row  14 :   versicolor versicolor     \n",
      "row  15 :    virginica virginica      \n",
      "row  16 :    virginica virginica      \n",
      "row  17 :    virginica virginica      \n",
      "row  18 :    virginica virginica      \n",
      "row  19 :   versicolor versicolor     \n",
      "row  20 :       setosa setosa         \n",
      "row  21 :       setosa setosa         \n",
      "row  22 :   versicolor versicolor     \n",
      "row  23 :   versicolor versicolor     \n",
      "row  24 :       setosa setosa         \n",
      "row  25 :   versicolor versicolor     \n",
      "row  26 :       setosa setosa         \n",
      "row  27 :   versicolor versicolor     \n",
      "\n",
      "Correct: 28 out of 28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Re-create and re-run the  \"Model-testing Cell\"     How does it do with best_k?!\n",
    "#\n",
    "predicted_labels = knn_model_tuned.predict(X_test)\n",
    "actual_labels = y_test\n",
    "\n",
    "# Let's print them so we can compare...\n",
    "print(\"Predicted labels:\", predicted_labels)\n",
    "print(\"Actual labels:\", actual_labels)\n",
    "print()\n",
    "# and, we'll print our nicer table...\n",
    "compare_labels(predicted_labels,actual_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created + trained a 'final' knn classifier, with a (best) k of 12\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Ok!  We have tuned knn to use the \"best\" value of k...\n",
    "#\n",
    "# And, we should really use ALL available data to train our final predictive model:\n",
    "#\n",
    "\n",
    "knn_model_final = KNeighborsClassifier(n_neighbors=best_k)   # here, we use the best_k\n",
    "knn_model_final.fit(X_all, y_all)                              # yay!  trained!\n",
    "print(f\"Created + trained a 'final' knn classifier, with a (best) k of {best_k}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I predict virginica (2) from Features [6.7, 3.3, 5.7, 2.1]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# final predictive model (k-nearest-neighbor), with tuned k + ALL data incorporated\n",
    "#\n",
    "\n",
    "def predictive_model( Features ):\n",
    "    \"\"\" input: a list of four features \n",
    "                [ sepallen, sepalwid, petallen, petalwid ]\n",
    "        output: the predicted species of iris, from\n",
    "                  setosa (0), versicolor (1), virginica (2)\n",
    "    \"\"\"\n",
    "    our_features = np.asarray([Features])                 # extra brackets needed\n",
    "    predicted_species = knn_model_final.predict(our_features)\n",
    "    \n",
    "    predicted_species = int(round(predicted_species[0]))  # unpack one element\n",
    "    name = SPECIES[predicted_species]\n",
    "    return f\"{name} ({predicted_species})\"\n",
    "    \n",
    "#\n",
    "# Try it!\n",
    "# \n",
    "# Features = eval(input(\"Enter new Features: \"))\n",
    "#\n",
    "Features = [6.7,3.3,5.7,2.1]  # [5.8,2.7,4.1,1.0] [4.6,3.6,3.0,2.2] [6.7,3.3,5.7,2.1]\n",
    "result = predictive_model( Features )\n",
    "print(f\"I predict {result} from Features {Features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I predict setosa (0) from Features [4.8, 3.1, 1.6, 0.2]\n",
      "I predict versicolor (1) from Features [5.7, 2.9, 4.2, 1.3]\n",
      "I predict virginica (2) from Features [5.8, 2.7, 5.1, 1.9]\n",
      "I predict setosa (0) from Features [5.2, 4.1, 1.5, 0.1]\n",
      "I predict setosa (0) from Features [5.4, 3.4, 1.5, 0.4]\n",
      "I predict versicolor (1) from Features [5.1, 2.5, 3.0, 1.1]\n",
      "I predict versicolor (1) from Features [6.2, 2.9, 4.3, 1.3]\n",
      "I predict virginica (2) from Features [6.3, 3.3, 6.0, 2.5]\n",
      "I predict versicolor (1) from Features [5.7, 2.8, 4.1, 1.3]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# try it on new, \"unseen\" data!\n",
    "#\n",
    "\n",
    "# Less unseen than in hw4, admittedly!\n",
    "\n",
    "LoF = [[4.8, 3.1, 1.6, 0.2 ],\n",
    "[5.7, 2.9, 4.2, 1.3 ],\n",
    "[5.8, 2.7, 5.1, 1.9 ],\n",
    "[5.2, 4.1, 1.5, 0.1 ],\n",
    "[5.4, 3.4, 1.5, 0.4 ],\n",
    "[5.1, 2.5, 3.0, 1.1 ],\n",
    "[6.2, 2.9, 4.3, 1.3 ],\n",
    "[6.3, 3.3, 6.0, 2.5 ],\n",
    "[5.7, 2.8, 4.1, 1.3 ]]\n",
    "      \n",
    "for Features in LoF:\n",
    "    result = predictive_model( Features )\n",
    "    print(f\"I predict {result} from Features {Features}\")\n",
    "\n",
    "# these flowers' coded species: 012001122"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Now, let's switch to a different model!!   (Decision Trees)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created and trained a DT classifier with max depth = 3\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# +++ This is a \"Model-building and Model-training Cell\"\n",
    "#       \n",
    "# Create a DT model and train it! \n",
    "#\n",
    "from sklearn import tree      # for decision trees\n",
    "\n",
    "best_depth = 3   # we don't know what depth to use, so we guess...\n",
    "dtree_model = tree.DecisionTreeClassifier(max_depth=best_depth)\n",
    "\n",
    "# we train the model (it's one line!)\n",
    "dtree_model.fit(X_train, y_train)                              # yay!  trained!\n",
    "print(\"Created and trained a DT classifier with max depth =\", best_depth) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels: [1. 1. 2. 0. 1. 2. 1. 1. 1. 0. 2. 0. 1. 2. 1. 2. 2. 2. 2. 1. 0. 0. 1. 1.\n",
      " 0. 1. 0. 1.]\n",
      "Actual  labels  : [1. 2. 2. 0. 1. 2. 1. 1. 1. 0. 2. 0. 1. 2. 1. 2. 2. 2. 2. 1. 0. 0. 1. 1.\n",
      " 0. 1. 0. 1.]\n",
      "\n",
      "Results on test set:  27 correct out of 28 total.\n",
      "row   0 :   versicolor versicolor     \n",
      "row   1 :   versicolor virginica      incorrect\n",
      "row   2 :    virginica virginica      \n",
      "row   3 :       setosa setosa         \n",
      "row   4 :   versicolor versicolor     \n",
      "row   5 :    virginica virginica      \n",
      "row   6 :   versicolor versicolor     \n",
      "row   7 :   versicolor versicolor     \n",
      "row   8 :   versicolor versicolor     \n",
      "row   9 :       setosa setosa         \n",
      "row  10 :    virginica virginica      \n",
      "row  11 :       setosa setosa         \n",
      "row  12 :   versicolor versicolor     \n",
      "row  13 :    virginica virginica      \n",
      "row  14 :   versicolor versicolor     \n",
      "row  15 :    virginica virginica      \n",
      "row  16 :    virginica virginica      \n",
      "row  17 :    virginica virginica      \n",
      "row  18 :    virginica virginica      \n",
      "row  19 :   versicolor versicolor     \n",
      "row  20 :       setosa setosa         \n",
      "row  21 :       setosa setosa         \n",
      "row  22 :   versicolor versicolor     \n",
      "row  23 :   versicolor versicolor     \n",
      "row  24 :       setosa setosa         \n",
      "row  25 :   versicolor versicolor     \n",
      "row  26 :       setosa setosa         \n",
      "row  27 :   versicolor versicolor     \n",
      "\n",
      "Correct: 27 out of 28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# +++ This is the \"Model-testing Cell\"\n",
    "#\n",
    "# Now, let's see how well we did on our \"held-out data\" (the testing data)\n",
    "#\n",
    "\n",
    "# We run our test set!\n",
    "predicted_labels = dtree_model.predict(X_test)\n",
    "actual_labels = y_test\n",
    "\n",
    "# Let's print them so we can compare...\n",
    "print(\"Predicted labels:\", predicted_labels)\n",
    "print(\"Actual  labels  :\", actual_labels)\n",
    "\n",
    "# And, some overall results\n",
    "num_correct = sum(predicted_labels == actual_labels)\n",
    "total = len(actual_labels)\n",
    "print(f\"\\nResults on test set:  {num_correct} correct out of {total} total.\")\n",
    "\n",
    "# and, let's print our table, too...\n",
    "compare_labels(predicted_labels,actual_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# file tree_depth_3.gv written. Try copying the result to http://viz-js.com/ \n",
      "\n",
      "digraph Tree {\n",
      "node [shape=box, style=\"filled\", color=\"black\"] ;\n",
      "graph [ranksep=equally, splines=polyline] ;\n",
      "0 [label=\"petalwid <= 0.8\\ngini = 0.665\\nsamples = 113\\nvalue = [40, 34, 39]\\nclass = setosa\", fillcolor=\"#fffdfc\"] ;\n",
      "1 [label=\"gini = 0.0\\nsamples = 40\\nvalue = [40, 0, 0]\\nclass = setosa\", fillcolor=\"#e58139\"] ;\n",
      "0 -> 1 [labeldistance=2.5, labelangle=45, headlabel=\"True\"] ;\n",
      "2 [label=\"petalwid <= 1.75\\ngini = 0.498\\nsamples = 73\\nvalue = [0, 34, 39]\\nclass = virginica\", fillcolor=\"#efe6fc\"] ;\n",
      "0 -> 2 [labeldistance=2.5, labelangle=-45, headlabel=\"False\"] ;\n",
      "3 [label=\"petallen <= 5.05\\ngini = 0.153\\nsamples = 36\\nvalue = [0, 33, 3]\\nclass = versicolor\", fillcolor=\"#4be78c\"] ;\n",
      "2 -> 3 ;\n",
      "4 [label=\"gini = 0.059\\nsamples = 33\\nvalue = [0, 32, 1]\\nclass = versicolor\", fillcolor=\"#3fe685\"] ;\n",
      "3 -> 4 ;\n",
      "5 [label=\"gini = 0.444\\nsamples = 3\\nvalue = [0, 1, 2]\\nclass = virginica\", fillcolor=\"#c09cf2\"] ;\n",
      "3 -> 5 ;\n",
      "6 [label=\"petallen <= 4.85\\ngini = 0.053\\nsamples = 37\\nvalue = [0, 1, 36]\\nclass = virginica\", fillcolor=\"#843ee6\"] ;\n",
      "2 -> 6 ;\n",
      "7 [label=\"gini = 0.444\\nsamples = 3\\nvalue = [0, 1, 2]\\nclass = virginica\", fillcolor=\"#c09cf2\"] ;\n",
      "6 -> 7 ;\n",
      "8 [label=\"gini = 0.0\\nsamples = 34\\nvalue = [0, 0, 34]\\nclass = virginica\", fillcolor=\"#8139e5\"] ;\n",
      "6 -> 8 ;\n",
      "{rank=same ; 0} ;\n",
      "{rank=same ; 2} ;\n",
      "{rank=same ; 3; 6} ;\n",
      "{rank=same ; 1; 4; 5; 7; 8} ;\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# let's see the tree!   Paste the tree's source into     http://viz-js.com/\n",
    "#\n",
    "\n",
    "filename = 'tree_depth_' + str(best_depth) + '.gv'  # preferred over .dot\n",
    "\n",
    "tree.export_graphviz(dtree_model, out_file=filename,  # the filename constructed above...!\n",
    "                            feature_names=COLUMNS[:-1], # actual feature names, not species\n",
    "                            filled=True,              # fun!\n",
    "                            rotate=False,             # False for Up/Down; True for L/R\n",
    "                            class_names=SPECIES,      # good to have   \n",
    "                            leaves_parallel=True )    # lots of options!\n",
    "\n",
    "print(f\"# file {filename} written. Try copying the result to http://viz-js.com/ \\n\")\n",
    "\n",
    "with open(filename, \"r\") as f:\n",
    "    file_text = f.read()\n",
    "    print(file_text)\n",
    "    \n",
    "#\n",
    "# Lab task:  build three trees at depths 1, 2, and 3 (submit with the notebooks!)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Now, to TUNE the model (with cross-validation)...\n",
    "#\n",
    "#\n",
    "# We used a depth of 1    (only 1 split) \n",
    "# There's no way to model three species with only 1 split!\n",
    "#\n",
    "# So, we try several depths...\n",
    "# Here, the tradeoff is not so much \"more accurate\" \n",
    "#       + deeper always has the potential to be more accurate\n",
    "#       + at the risk of overfitting the training data!\n",
    "#\n",
    "# Rather it's the underfitting(bias)/overfitting(variance) tradeoff\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++best so fair!!!+++++\n",
      "d:  1  cv accuracy:  0.6992\n",
      "+++++best so fair!!!+++++\n",
      "d:  2  cv accuracy:  0.9474\n",
      "d:  3  cv accuracy:  0.9119\n",
      "d:  4  cv accuracy:  0.9383\n",
      "d:  5  cv accuracy:  0.9383\n",
      "\n",
      "best_depth = 2 is our choice for underfitting/overfitting balance.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# So, to compare different depths, let's use cross validation\n",
    "#\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#\n",
    "# cross-validation splits the training set into two pieces:\n",
    "#   + model-building and model-validation. We'll use \"build\" and \"validate\"\n",
    "#\n",
    "\n",
    "best_depth= 0\n",
    "best_acc=0\n",
    "\n",
    "\n",
    "for d in range(1,6):\n",
    "    cv_model = tree.DecisionTreeClassifier(max_depth=d)   # for each depth, d\n",
    "    cv_scores = cross_val_score( cv_model, X_train, y_train, cv=5 ) # 5 means 80/20 split\n",
    "    # print(cv_scores)  # if we want to see the five individual scores \n",
    "    average_cv_accuracy = cv_scores.mean()  # more likely, only their average\n",
    "    if average_cv_accuracy > best_acc:\n",
    "        best_depth = d\n",
    "        best_acc= average_cv_accuracy\n",
    "        print(\"+++++best so fair!!!+++++\")\n",
    "    print(f\"d: {d:2d}  cv accuracy: {average_cv_accuracy:7.4f}\")\n",
    "      \n",
    "    \n",
    "# assign best value of d to best_depth\n",
    "  \n",
    "print()\n",
    "print(f\"best_depth = {best_depth} is our choice for underfitting/overfitting balance.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created and trained a DT classifier with max depth = 2\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Now, we re-create and re-run the  \"Model-building and -training Cell\"\n",
    "#\n",
    "# Now, using the tuned value...\n",
    "#\n",
    "from sklearn import tree      # for decision trees\n",
    "\n",
    "# we should have best_depth from our cv exploration\n",
    "dtree_model_tuned = tree.DecisionTreeClassifier(max_depth=best_depth)\n",
    "\n",
    "# we train the model (it's one line!)\n",
    "dtree_model_tuned.fit(X_train, y_train)                              # yay!  trained!\n",
    "print(\"Created and trained a DT classifier with max depth =\", best_depth) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels: [1. 1. 1. 0. 1. 2. 1. 1. 1. 0. 2. 0. 1. 2. 1. 2. 2. 2. 2. 1. 0. 0. 1. 1.\n",
      " 0. 1. 0. 1.]\n",
      "Actual labels: [1. 2. 2. 0. 1. 2. 1. 1. 1. 0. 2. 0. 1. 2. 1. 2. 2. 2. 2. 1. 0. 0. 1. 1.\n",
      " 0. 1. 0. 1.]\n",
      "\n",
      "row   0 :   versicolor versicolor     \n",
      "row   1 :   versicolor virginica      incorrect\n",
      "row   2 :   versicolor virginica      incorrect\n",
      "row   3 :       setosa setosa         \n",
      "row   4 :   versicolor versicolor     \n",
      "row   5 :    virginica virginica      \n",
      "row   6 :   versicolor versicolor     \n",
      "row   7 :   versicolor versicolor     \n",
      "row   8 :   versicolor versicolor     \n",
      "row   9 :       setosa setosa         \n",
      "row  10 :    virginica virginica      \n",
      "row  11 :       setosa setosa         \n",
      "row  12 :   versicolor versicolor     \n",
      "row  13 :    virginica virginica      \n",
      "row  14 :   versicolor versicolor     \n",
      "row  15 :    virginica virginica      \n",
      "row  16 :    virginica virginica      \n",
      "row  17 :    virginica virginica      \n",
      "row  18 :    virginica virginica      \n",
      "row  19 :   versicolor versicolor     \n",
      "row  20 :       setosa setosa         \n",
      "row  21 :       setosa setosa         \n",
      "row  22 :   versicolor versicolor     \n",
      "row  23 :   versicolor versicolor     \n",
      "row  24 :       setosa setosa         \n",
      "row  25 :   versicolor versicolor     \n",
      "row  26 :       setosa setosa         \n",
      "row  27 :   versicolor versicolor     \n",
      "\n",
      "Correct: 26 out of 28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Re-create and re-run the  \"Model-testing Cell\"     How does it do with best_k?!\n",
    "#\n",
    "predicted_labels = dtree_model_tuned.predict(X_test)\n",
    "actual_labels = y_test\n",
    "\n",
    "# Let's print them so we can compare...\n",
    "print(\"Predicted labels:\", predicted_labels)\n",
    "print(\"Actual labels:\", actual_labels)\n",
    "print()\n",
    "\n",
    "# and, we'll print our nicer table...\n",
    "compare_labels(predicted_labels,actual_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created and trained a 'final' DT classifier with max depth = 2\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Ok!  We have tuned our DT to use the \"best\" depth...\n",
    "#\n",
    "# Now, we use ALL available data to train our final predictive model:\n",
    "#\n",
    "\n",
    "from sklearn import tree      # for decision trees\n",
    "\n",
    "# we should have best_depth from our cv exploration\n",
    "dtree_model_final = tree.DecisionTreeClassifier(max_depth=best_depth)\n",
    "\n",
    "# we train the model (it's one line!)\n",
    "dtree_model_final.fit(X_all, y_all)                              # yay!  trained!\n",
    "print(\"Created and trained a 'final' DT classifier with max depth =\", best_depth) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I predict setosa (0) from Features [4.6, 3.1, 2.0, 1.5]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# final predictive model (k-nearest-neighbor), with tuned k + ALL data incorporated\n",
    "#\n",
    "\n",
    "def predictive_model( Features ):\n",
    "    \"\"\" input: a list of four features \n",
    "                [ sepallen, sepalwid, petallen, petalwid ]\n",
    "        output: the predicted species of iris, from\n",
    "                  setosa (0), versicolor (1), virginica (2)\n",
    "    \"\"\"\n",
    "    our_features = np.asarray([Features])                 # extra brackets needed\n",
    "    predicted_species = dtree_model_final.predict(our_features)\n",
    "    \n",
    "    predicted_species = int(round(predicted_species[0]))  # unpack one element\n",
    "    name = SPECIES[predicted_species]\n",
    "    return f\"{name} ({predicted_species})\"\n",
    "    \n",
    "#\n",
    "# Try it!\n",
    "# \n",
    "# Features = eval(input(\"Enter new Features: \"))\n",
    "#\n",
    "Features = [4.6,3.1,2.0,1.5]   # [4.8,2.7,3.1,0.2] [4.6,3.1,2.0,1.5] [6.7,3.3,5.7,2.1]\n",
    "result = predictive_model( Features )\n",
    "print(f\"I predict {result} from Features {Features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I predict setosa (0) from Features [4.8, 3.1, 1.6, 0.2]\n",
      "I predict versicolor (1) from Features [5.7, 2.9, 4.2, 1.3]\n",
      "I predict virginica (2) from Features [5.8, 2.7, 5.1, 1.9]\n",
      "I predict setosa (0) from Features [5.2, 4.1, 1.5, 0.1]\n",
      "I predict setosa (0) from Features [5.4, 3.4, 1.5, 0.4]\n",
      "I predict versicolor (1) from Features [5.1, 2.5, 3.0, 1.1]\n",
      "I predict versicolor (1) from Features [6.2, 2.9, 4.3, 1.3]\n",
      "I predict virginica (2) from Features [6.3, 3.3, 6.0, 2.5]\n",
      "I predict versicolor (1) from Features [5.7, 2.8, 4.1, 1.3]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# try it on new, \"unseen\" data!\n",
    "#\n",
    "\n",
    "# Less unseen than in hw4, admittedly!\n",
    "\n",
    "LoF = [[4.8, 3.1, 1.6, 0.2 ],\n",
    "[5.7, 2.9, 4.2, 1.3 ],\n",
    "[5.8, 2.7, 5.1, 1.9 ],\n",
    "[5.2, 4.1, 1.5, 0.1 ],\n",
    "[5.4, 3.4, 1.5, 0.4 ],\n",
    "[5.1, 2.5, 3.0, 1.1 ],\n",
    "[6.2, 2.9, 4.3, 1.3 ],\n",
    "[6.3, 3.3, 6.0, 2.5 ],\n",
    "[5.7, 2.8, 4.1, 1.3 ]]\n",
    "      \n",
    "for Features in LoF:\n",
    "    result = predictive_model( Features )\n",
    "    print(f\"I predict {result} from Features {Features}\")\n",
    "\n",
    "# these flowers' coded species: 012001122"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.56613722 0.43386278]\n",
      "\n",
      "Feature     sepallen has    0.00% of the decision-making importance.\n",
      "Feature     sepalwid has    0.00% of the decision-making importance.\n",
      "Feature     petallen has   56.61% of the decision-making importance.\n",
      "Feature     petalwid has   43.39% of the decision-making importance.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# feature importances can be even more \"important\" than predictions!\n",
    "#\n",
    "\n",
    "print(dtree_model_final.feature_importances_)\n",
    "print()\n",
    "\n",
    "# let's see them with each feature name:\n",
    "IMPs = dtree_model_final.feature_importances_\n",
    "\n",
    "# enumerate is great when you want indices _and_ elements!\n",
    "for i, importance in enumerate(IMPs):\n",
    "    perc = importance*100\n",
    "    print(f\"Feature {COLUMNS[i]:>12s} has {perc:>7.2f}% of the decision-making importance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# That's it!  Welcome to the world of Decision-Tree models!    \n",
    "#\n",
    "\n",
    "#\n",
    "# But wait, there's more!  More workflows, and more trees!  Random Forests next:\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Random Forests!!!\n",
    "#\n",
    "\n",
    "# Lots of trees, each using a partial fraction of the data\n",
    "#      that get together to vote on the correct classification..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built an RF with depth=3 and #trees=42\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# +++ This is the \"Model-building and Model-training Cell\"\n",
    "#       \n",
    "# Create a RF model and train it! \n",
    "#\n",
    "from sklearn import tree      # for decision trees\n",
    "from sklearn import ensemble  # for random forests\n",
    "\n",
    "best_depth = 3        # we don't know what depth to use, so we guess...\n",
    "best_num_trees = 42   # again, we guess\n",
    "rforest_model = ensemble.RandomForestClassifier(max_depth=best_depth, \n",
    "                                                n_estimators=best_num_trees)\n",
    "\n",
    "# we train the model (it's one line!)\n",
    "rforest_model.fit(X_train, y_train)                              # yay!  trained!\n",
    "print(f\"Built an RF with depth={best_depth} and #trees={best_num_trees}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels: [1. 1. 2. 0. 1. 2. 1. 1. 1. 0. 2. 0. 1. 2. 1. 2. 2. 2. 2. 1. 0. 0. 1. 1.\n",
      " 0. 1. 0. 1.]\n",
      "Actual  labels  : [1. 2. 2. 0. 1. 2. 1. 1. 1. 0. 2. 0. 1. 2. 1. 2. 2. 2. 2. 1. 0. 0. 1. 1.\n",
      " 0. 1. 0. 1.]\n",
      "\n",
      "Results on test set:  27 correct out of 28 total.\n",
      "row   0 :   versicolor versicolor     \n",
      "row   1 :   versicolor virginica      incorrect\n",
      "row   2 :    virginica virginica      \n",
      "row   3 :       setosa setosa         \n",
      "row   4 :   versicolor versicolor     \n",
      "row   5 :    virginica virginica      \n",
      "row   6 :   versicolor versicolor     \n",
      "row   7 :   versicolor versicolor     \n",
      "row   8 :   versicolor versicolor     \n",
      "row   9 :       setosa setosa         \n",
      "row  10 :    virginica virginica      \n",
      "row  11 :       setosa setosa         \n",
      "row  12 :   versicolor versicolor     \n",
      "row  13 :    virginica virginica      \n",
      "row  14 :   versicolor versicolor     \n",
      "row  15 :    virginica virginica      \n",
      "row  16 :    virginica virginica      \n",
      "row  17 :    virginica virginica      \n",
      "row  18 :    virginica virginica      \n",
      "row  19 :   versicolor versicolor     \n",
      "row  20 :       setosa setosa         \n",
      "row  21 :       setosa setosa         \n",
      "row  22 :   versicolor versicolor     \n",
      "row  23 :   versicolor versicolor     \n",
      "row  24 :       setosa setosa         \n",
      "row  25 :   versicolor versicolor     \n",
      "row  26 :       setosa setosa         \n",
      "row  27 :   versicolor versicolor     \n",
      "\n",
      "Correct: 27 out of 28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# +++ This is the \"Model-testing Cell\"\n",
    "#\n",
    "# Now, let's see how well we did on our \"held-out data\" (the testing data)\n",
    "#\n",
    "\n",
    "# We run our test set!\n",
    "predicted_labels = rforest_model.predict(X_test)\n",
    "actual_labels = y_test\n",
    "\n",
    "# Let's print them so we can compare...\n",
    "print(\"Predicted labels:\", predicted_labels)\n",
    "print(\"Actual  labels  :\", actual_labels)\n",
    "\n",
    "# And, some overall results\n",
    "num_correct = sum(predicted_labels == actual_labels)\n",
    "total = len(actual_labels)\n",
    "print(f\"\\nResults on test set:  {num_correct} correct out of {total} total.\")\n",
    "\n",
    "# and, let's print our table, too...\n",
    "compare_labels(predicted_labels,actual_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the forest's trees is DecisionTreeClassifier(max_depth=3, max_features='auto',\n",
      "                       random_state=1621357769)\n",
      "# file rf_tree_024.gv written. Try copying the result to http://viz-js.com/ \n",
      "\n",
      "digraph Tree {\n",
      "node [shape=box, style=\"filled\", color=\"black\"] ;\n",
      "graph [ranksep=equally, splines=polyline] ;\n",
      "0 [label=\"sepallen <= 5.45\\ngini = 0.665\\nsamples = 71\\nvalue = [39, 34, 40]\\nclass = virginica\", fillcolor=\"#fdfcff\"] ;\n",
      "1 [label=\"petalwid <= 0.9\\ngini = 0.149\\nsamples = 23\\nvalue = [34, 3, 0]\\nclass = setosa\", fillcolor=\"#e78c4a\"] ;\n",
      "0 -> 1 [labeldistance=2.5, labelangle=45, headlabel=\"True\"] ;\n",
      "2 [label=\"gini = 0.0\\nsamples = 21\\nvalue = [34, 0, 0]\\nclass = setosa\", fillcolor=\"#e58139\"] ;\n",
      "1 -> 2 ;\n",
      "3 [label=\"gini = 0.0\\nsamples = 2\\nvalue = [0, 3, 0]\\nclass = versicolor\", fillcolor=\"#39e581\"] ;\n",
      "1 -> 3 ;\n",
      "4 [label=\"sepallen <= 6.15\\ngini = 0.552\\nsamples = 48\\nvalue = [5, 31, 40]\\nclass = virginica\", fillcolor=\"#e6d7fa\"] ;\n",
      "0 -> 4 [labeldistance=2.5, labelangle=-45, headlabel=\"False\"] ;\n",
      "5 [label=\"sepalwid <= 3.6\\ngini = 0.414\\nsamples = 20\\nvalue = [5, 23, 3]\\nclass = versicolor\", fillcolor=\"#76eda8\"] ;\n",
      "4 -> 5 ;\n",
      "6 [label=\"gini = 0.204\\nsamples = 17\\nvalue = [0, 23, 3]\\nclass = versicolor\", fillcolor=\"#53e891\"] ;\n",
      "5 -> 6 ;\n",
      "7 [label=\"gini = 0.0\\nsamples = 3\\nvalue = [5, 0, 0]\\nclass = setosa\", fillcolor=\"#e58139\"] ;\n",
      "5 -> 7 ;\n",
      "8 [label=\"petalwid <= 1.75\\ngini = 0.292\\nsamples = 28\\nvalue = [0, 8, 37]\\nclass = virginica\", fillcolor=\"#9c64eb\"] ;\n",
      "4 -> 8 ;\n",
      "9 [label=\"gini = 0.198\\nsamples = 8\\nvalue = [0, 8, 1]\\nclass = versicolor\", fillcolor=\"#52e891\"] ;\n",
      "8 -> 9 ;\n",
      "10 [label=\"gini = 0.0\\nsamples = 20\\nvalue = [0, 0, 36]\\nclass = virginica\", fillcolor=\"#8139e5\"] ;\n",
      "8 -> 10 ;\n",
      "{rank=same ; 0} ;\n",
      "{rank=same ; 1; 4} ;\n",
      "{rank=same ; 5; 8} ;\n",
      "{rank=same ; 2; 3; 6; 7; 9; 10} ;\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# we can get the individual trees, if we want...  Paste source into http://viz-js.com/\n",
    "#\n",
    "i = 24\n",
    "one_rf_tree = rforest_model.estimators_[i]\n",
    "print(f\"One of the forest's trees is {one_rf_tree}\")\n",
    "\n",
    "# From there, it's possible to create a graphical version...\n",
    "filename = f'rf_tree_{i:03d}.gv'    # .gv preferred over .dot\n",
    "tree.export_graphviz(one_rf_tree, out_file=filename,  # the filename constructed above...!\n",
    "                            feature_names=COLUMNS[:-1], # actual feature names, not species\n",
    "                            filled=True,              # fun!\n",
    "                            rotate=False,             # False for Up/Down; True for L/R\n",
    "                            class_names=SPECIES,      # good to have   \n",
    "                            leaves_parallel=True )    # lots of options!\n",
    "print(f\"# file {filename} written. Try copying the result to http://viz-js.com/ \\n\")\n",
    "with open(filename, \"r\") as f:\n",
    "    file_text = f.read()\n",
    "    print(file_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Now, to TUNE the model (with cross-validation)...\n",
    "#\n",
    "#\n",
    "# We used a depth of 1  and #trees of 42  \n",
    "#\n",
    "# So, we try several depths and # of trees\n",
    "# \n",
    "# Again, the tradeoff is underfitting/overfitting...\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth:  1 ntrees:  50 cv accuracy:  0.8929\n",
      "depth:  2 ntrees:  50 cv accuracy:  0.9387\n",
      "depth:  3 ntrees:  50 cv accuracy:  0.9474\n",
      "depth:  4 ntrees:  50 cv accuracy:  0.9565\n",
      "depth:  5 ntrees:  50 cv accuracy:  0.9474\n",
      "depth:  1 ntrees: 150 cv accuracy:  0.8743\n",
      "depth:  2 ntrees: 150 cv accuracy:  0.9474\n",
      "depth:  3 ntrees: 150 cv accuracy:  0.9474\n",
      "depth:  4 ntrees: 150 cv accuracy:  0.9474\n",
      "depth:  5 ntrees: 150 cv accuracy:  0.9565\n",
      "depth:  1 ntrees: 250 cv accuracy:  0.7960\n",
      "depth:  2 ntrees: 250 cv accuracy:  0.9387\n",
      "depth:  3 ntrees: 250 cv accuracy:  0.9474\n",
      "depth:  4 ntrees: 250 cv accuracy:  0.9474\n",
      "depth:  5 ntrees: 250 cv accuracy:  0.9474\n",
      "\n",
      "best_depth: 4 and best_num_trees: 50 are our choices.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# So, to compare different parameters, let's use cv\n",
    "#\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#\n",
    "# cross-validation splits the training set into two pieces:\n",
    "#   + model-building and model-validation. We'll use \"build\" and \"validate\"\n",
    "#\n",
    "\n",
    "#\n",
    "# lab task:  wrap this loop in another one! (or create an inner one...)\n",
    "#\n",
    "\n",
    "best_depth = 0   \n",
    "best_num_trees = 0\n",
    "best_acc= 0\n",
    "\n",
    "\n",
    "for ntrees in range(50,300,100):\n",
    "    \n",
    "    for d in range(1,6):\n",
    "        rforest_model = ensemble.RandomForestClassifier(max_depth=d, \n",
    "                                                        n_estimators=ntrees)\n",
    "        cv_scores = cross_val_score( rforest_model, X_train, y_train, cv=5 ) # 5 means 80/20 split\n",
    "        # print(cv_scores)  # if we want to see the five individual scores \n",
    "        average_cv_accuracy = cv_scores.mean()  # more likely, only their average\n",
    "        if average_cv_accuracy>best_acc:\n",
    "            best_acc=average_cv_accuracy\n",
    "            best_depth= d\n",
    "            best_num_trees= ntrees\n",
    "        print(f\"depth: {d:2d} ntrees: {ntrees:3d} cv accuracy: {average_cv_accuracy:7.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# assign best values\n",
    "\n",
    "print()\n",
    "print(f\"best_depth: {best_depth} and best_num_trees: {best_num_trees} are our choices.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built an RF classifier with depth=4 and ntrees=50\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Now, we re-create and re-run the  \"Model-building and -training Cell\"\n",
    "#\n",
    "from sklearn import tree      # for decision trees\n",
    "from sklearn import ensemble  # for random forests\n",
    "\n",
    "# we should have best_depth and best_num_trees\n",
    "rforest_model_tuned = ensemble.RandomForestClassifier(max_depth=best_depth, \n",
    "                                                      n_estimators=best_num_trees)\n",
    "\n",
    "# we train the model (it's one line!)\n",
    "rforest_model_tuned.fit(X_train, y_train)                              # yay!  trained!\n",
    "print(f\"Built an RF classifier with depth={best_depth} and ntrees={best_num_trees}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels: [1. 1. 1. 0. 1. 2. 1. 1. 1. 0. 2. 0. 1. 2. 1. 2. 2. 2. 2. 1. 0. 0. 1. 1.\n",
      " 0. 1. 0. 1.]\n",
      "Actual  labels  : [1. 2. 2. 0. 1. 2. 1. 1. 1. 0. 2. 0. 1. 2. 1. 2. 2. 2. 2. 1. 0. 0. 1. 1.\n",
      " 0. 1. 0. 1.]\n",
      "\n",
      "Results on test set:  26 correct out of 28 total.\n",
      "row   0 :   versicolor versicolor     \n",
      "row   1 :   versicolor virginica      incorrect\n",
      "row   2 :   versicolor virginica      incorrect\n",
      "row   3 :       setosa setosa         \n",
      "row   4 :   versicolor versicolor     \n",
      "row   5 :    virginica virginica      \n",
      "row   6 :   versicolor versicolor     \n",
      "row   7 :   versicolor versicolor     \n",
      "row   8 :   versicolor versicolor     \n",
      "row   9 :       setosa setosa         \n",
      "row  10 :    virginica virginica      \n",
      "row  11 :       setosa setosa         \n",
      "row  12 :   versicolor versicolor     \n",
      "row  13 :    virginica virginica      \n",
      "row  14 :   versicolor versicolor     \n",
      "row  15 :    virginica virginica      \n",
      "row  16 :    virginica virginica      \n",
      "row  17 :    virginica virginica      \n",
      "row  18 :    virginica virginica      \n",
      "row  19 :   versicolor versicolor     \n",
      "row  20 :       setosa setosa         \n",
      "row  21 :       setosa setosa         \n",
      "row  22 :   versicolor versicolor     \n",
      "row  23 :   versicolor versicolor     \n",
      "row  24 :       setosa setosa         \n",
      "row  25 :   versicolor versicolor     \n",
      "row  26 :       setosa setosa         \n",
      "row  27 :   versicolor versicolor     \n",
      "\n",
      "Correct: 26 out of 28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# +++ This is our \"Model-testing Cell\"\n",
    "#\n",
    "# Now, let's see how well we did on our \"held-out data\" (the testing data)\n",
    "#\n",
    "\n",
    "# We run our test set!\n",
    "predicted_labels = rforest_model_tuned.predict(X_test)\n",
    "actual_labels = y_test\n",
    "\n",
    "# Let's print them so we can compare...\n",
    "print(\"Predicted labels:\", predicted_labels)\n",
    "print(\"Actual  labels  :\", actual_labels)\n",
    "\n",
    "# And, some overall results\n",
    "num_correct = sum(predicted_labels == actual_labels)\n",
    "total = len(actual_labels)\n",
    "print(f\"\\nResults on test set:  {num_correct} correct out of {total} total.\")\n",
    "\n",
    "# and, let's print our table, too...\n",
    "compare_labels(predicted_labels,actual_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built an RF classifier with depth=4 and ntrees=50\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Ok!  We have tuned our RF to use the \"best\" parameters\n",
    "#\n",
    "# Now, we use ALL available data to train our final predictive model:\n",
    "#\n",
    "from sklearn import tree      # for decision trees\n",
    "from sklearn import ensemble  # for random forests\n",
    "\n",
    "# we should have best_depth and best_num_trees\n",
    "rforest_model_final = ensemble.RandomForestClassifier(max_depth=best_depth, \n",
    "                                                      n_estimators=best_num_trees)\n",
    "\n",
    "# we train the model (it's one line!)\n",
    "rforest_model_final.fit(X_all, y_all)              # yay!  trained!\n",
    "print(f\"Built an RF classifier with depth={best_depth} and ntrees={best_num_trees}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I predict virginica (2) from Features [6.7, 3.3, 5.7, 2.1]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# final predictive model (k-nearest-neighbor), with tuned k + ALL data incorporated\n",
    "#\n",
    "\n",
    "def predictive_model( Features ):\n",
    "    \"\"\" input: a list of four features \n",
    "                [ sepallen, sepalwid, petallen, petalwid ]\n",
    "        output: the predicted species of iris, from\n",
    "                  setosa (0), versicolor (1), virginica (2)\n",
    "    \"\"\"\n",
    "    our_features = np.asarray([Features])                 # extra brackets needed\n",
    "    predicted_species = rforest_model_final.predict(our_features)\n",
    "    \n",
    "    predicted_species = int(round(predicted_species[0]))  # unpack one element\n",
    "    name = SPECIES[predicted_species]\n",
    "    return f\"{name} ({predicted_species})\"\n",
    "    \n",
    "#\n",
    "# Try it!\n",
    "# \n",
    "# Features = eval(input(\"Enter new Features: \"))\n",
    "#\n",
    "Features = [6.7,3.3,5.7,2.1]   # [4.8,2.7,3.1,0.2] [4.6,3.1,2.0,1.5] [6.7,3.3,5.7,2.1]\n",
    "result = predictive_model( Features )\n",
    "print(f\"I predict {result} from Features {Features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I predict setosa (0) from Features [4.8, 3.1, 1.6, 0.2]\n",
      "I predict versicolor (1) from Features [5.7, 2.9, 4.2, 1.3]\n",
      "I predict virginica (2) from Features [5.8, 2.7, 5.1, 1.9]\n",
      "I predict setosa (0) from Features [5.2, 4.1, 1.5, 0.1]\n",
      "I predict setosa (0) from Features [5.4, 3.4, 1.5, 0.4]\n",
      "I predict versicolor (1) from Features [5.1, 2.5, 3.0, 1.1]\n",
      "I predict versicolor (1) from Features [6.2, 2.9, 4.3, 1.3]\n",
      "I predict virginica (2) from Features [6.3, 3.3, 6.0, 2.5]\n",
      "I predict versicolor (1) from Features [5.7, 2.8, 4.1, 1.3]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# try it on new, \"unseen\" data!\n",
    "#\n",
    "\n",
    "# Less unseen than in hw4, admittedly!\n",
    "\n",
    "LoF = [[4.8, 3.1, 1.6, 0.2 ],\n",
    "[5.7, 2.9, 4.2, 1.3 ],\n",
    "[5.8, 2.7, 5.1, 1.9 ],\n",
    "[5.2, 4.1, 1.5, 0.1 ],\n",
    "[5.4, 3.4, 1.5, 0.4 ],\n",
    "[5.1, 2.5, 3.0, 1.1 ],\n",
    "[6.2, 2.9, 4.3, 1.3 ],\n",
    "[6.3, 3.3, 6.0, 2.5 ],\n",
    "[5.7, 2.8, 4.1, 1.3 ]]\n",
    "      \n",
    "for Features in LoF:\n",
    "    result = predictive_model( Features )\n",
    "    print(f\"I predict {result} from Features {Features}\")\n",
    "\n",
    "# these flowers' coded species: 012001122"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05456228 0.01337842 0.43721505 0.49484425]\n",
      "\n",
      "Feature     sepallen has    5.46% of the decision-making importance.\n",
      "Feature     sepalwid has    1.34% of the decision-making importance.\n",
      "Feature     petallen has   43.72% of the decision-making importance.\n",
      "Feature     petalwid has   49.48% of the decision-making importance.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# feature importances can be even more \"important\" than predictions!\n",
    "#\n",
    "\n",
    "print(rforest_model_final.feature_importances_)\n",
    "print()\n",
    "\n",
    "# let's see them with each feature name:\n",
    "IMPs = rforest_model_final.feature_importances_\n",
    "\n",
    "# enumerate is great when you want indices _and_ elements!\n",
    "for i, importance in enumerate(IMPs):\n",
    "    perc = importance*100\n",
    "    print(f\"Feature {COLUMNS[i]:>12s} has {perc:>7.2f}% of the decision-making importance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
